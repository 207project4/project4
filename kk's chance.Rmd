---
title: "Predictive Power Comparision Between Logistic Regression and Random Forests"
output:
  pdf_document: default
  fontsize: 8pt
  html_document:
    df_print: paged
    fig_caption: yes
    number_sections: yes
---
<style type="text/css">

body{ /* Normal  */
      font-size: 18px;
  }

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,message=FALSE,warning=FALSE)
```


Team ID: 3

Name (writing): Kieran Zhou

Name (writing): Liela Meng

Name (coding): Yunan Hou

Name (coding): Zhen Li

Github repo link: https://github.com/207project4/project4.git



# 1.Introduction
--

# 2.exploratory data analysis



```{r, message=F,include=F}
library(gmodels) # Cross Tables [CrossTable()]
library(ggmosaic) # Mosaic plot with ggplot [geom_mosaic()]
library(corrplot) # Correlation plot [corrplot()]
library(ggpubr) # Arranging ggplots together [ggarrange()]
library(cowplot) # Arranging ggplots together [plot_grid()]
library(caret) # ML [train(), confusionMatrix(), createDataPartition(), varImp(), trainControl()]
library(ROCR) # Model performance [performance(), prediction()]
library(plotROC) # ROC Curve with ggplot [geom_roc()]
library(pROC) # AUC computation [auc()]
library(PRROC) # AUPR computation [pr.curve()]
library(rpart) # Decision trees [rpart(), plotcp(), prune()]
library(rpart.plot) # Decision trees plotting [rpart.plot()]
library(ranger) # Optimized Random Forest [ranger()]
#library(lightgbm) # Light GBM [lgb.train()]
library(xgboost) # XGBoost [xgb.DMatrix(), xgb.train()]
library(MLmetrics) # Custom metrics (F1 score for example)
library(tidyverse) # Data manipulation
library(e1071)#for confusionMatrix
#library(doMC) # Parallel processing
#registerDoMC(cores = 10)
```
```{r}
# default theme for ggplot
theme_set(theme_bw())

# setting default parameters for mosaic plots
mosaic_theme = theme(axis.text.x = element_text(angle = 90,
                                                hjust = 1,
                                                vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank())

# setting default parameters for crosstables
fun_crosstable = function(df, var1, var2){
  # df: dataframe containing both columns to cross
  # var1, var2: columns to cross together.
  CrossTable(df[, var1], df[, var2],
             prop.r = T,
             prop.c = F,
             prop.t = F,
             prop.chisq = F,
             dnn = c(var1, var2))
}

# plot weighted lm/leoss regressions with frequencies
fun_gg_freq = function(var){
  # var: which column from bank_data to use in regressions
  
  # computing weights first...
  weight = table(bank_data[, var]) %>% 
    as.data.frame %>% 
    mutate(x = as.numeric(as.character(Var1))) %>% 
    select(-Var1) %>% 
    rename(weight = Freq)
  
  # ... then frequencies
  sink(tempfile())
  freq = fun_crosstable(bank_data, var, "y")$prop.r %>% 
    as.data.frame %>% 
    mutate(x = as.numeric(as.character(x)))
  sink()
  
  # assembling
  both = freq %>% 
    left_join(weight, by = "x") %>% 
    filter(weight > 50 & y == 1)
  
  # plotting
  gg = both %>% 
    ggplot() +
    aes(x = x,
        y = Freq,
        weight = weight) +
    geom_point(aes(size = weight)) +
    geom_smooth(aes(colour = "blue"), method = "loess") +
    geom_smooth(aes(colour = "red"), method = "lm", se = F) +
    coord_cartesian(ylim = c(-0.1, 1)) +
    theme(plot.margin = unit(c(0, 0, 0, 0), "pt")) +
    xlab(var) +
    ylab("") +
    scale_x_continuous(position = "top") +
    scale_colour_manual(values = c("blue", "red"),
                        labels = c("loess", "lm")) +
    labs(colour = "Regression")
  
  return(gg)
}

# re-ordering levels from factor variable
fun_reorder_levels = function(df, variable, first){
  # df: dataframe containing columns to transform into factors
  # variable: variable to transform into factor
  # first: first level of the variable to transform.
  
  remaining = unique(df[, variable])[which(unique(df[, variable]) != first)]
  x = factor(df[, variable], levels = c(first, remaining))
  return(x)
}

# plotting importance from predictive models into two panels
fun_imp_ggplot_split = function(model){
  # model: model used to plot variable importances
  
  if (class(model)[1] == "ranger"){
    imp_df = model$variable.importance %>% 
      data.frame("Overall" = .) %>% 
      rownames_to_column() %>% 
      rename(variable = rowname) %>% 
      arrange(-Overall)
  } else {
    imp_df = varImp(model) %>%
      rownames_to_column() %>% 
      rename(variable = rowname) %>% 
      arrange(-Overall)
  }
  
  # first panel (half most important variables)
  gg1 = imp_df %>% 
    slice(1:floor(nrow(.)/2)) %>% 
    ggplot() +
    aes(x = reorder(variable, Overall), weight = Overall, fill = -Overall) +
    geom_bar() +
    coord_flip() +
    xlab("Variables") +
    ylab("Importance") +
    theme(legend.position = "none")
  
  imp_range = ggplot_build(gg1)[["layout"]][["panel_params"]][[1]][["x.range"]]
  imp_gradient = scale_fill_gradient(limits = c(-imp_range[2], -imp_range[1]),
                                     low = "#132B43", 
                                     high = "#56B1F7")
  
  # second panel (less important variables)
  gg2 = imp_df %>% 
    slice(floor(nrow(.)/2)+1:nrow(.)) %>% 
    ggplot() +
    aes(x = reorder(variable, Overall), weight = Overall, fill = -Overall) +
    geom_bar() +
    coord_flip() +
    xlab("") +
    ylab("Importance") +
    theme(legend.position = "none") +
    ylim(imp_range) +
    imp_gradient
  
  # arranging together
  gg_both = plot_grid(gg1 + imp_gradient,
                      gg2)
  
  return(gg_both)
}

# plotting two performance measures
fun_gg_cutoff = function(score, obs, measure1, measure2) {
  # score: predicted scores
  # obs: real classes
  # measure1, measure2: which performance metrics to plot
  
  predictions = prediction(score, obs)
  performance1 = performance(predictions, measure1)
  performance2 = performance(predictions, measure2)
  
  df1 = data.frame(x = performance1@x.values[[1]],
                   y = performance1@y.values[[1]],
                   measure = measure1,
                   stringsAsFactors = F) %>% 
    drop_na()
  df2 = data.frame(x = performance2@x.values[[1]],
                   y = performance2@y.values[[1]],
                   measure = measure2,
                   stringsAsFactors = F) %>% 
    drop_na()
  
  # df contains all the data needed to plot both curves
  df = df1 %>% 
    bind_rows(df2)
  
  # extracting best cut for each measure
  y_max_measure1 = max(df1$y, na.rm = T)
  x_max_measure1 = df1[df1$y == y_max_measure1, "x"][1]
  
  y_max_measure2 = max(df2$y, na.rm = T)
  x_max_measure2 = df2[df2$y == y_max_measure2, "x"][1]
  
  txt_measure1 = paste("Best cut for", measure1, ": x =", round(x_max_measure1, 3))
  txt_measure2 = paste("Best cut for", measure2, ": x =", round(x_max_measure2, 3))
  txt_tot = paste(txt_measure1, "\n", txt_measure2, sep = "")
  
  # plotting both measures in the same plot, with some detail around.
  gg = df %>% 
    ggplot() +
    aes(x = x,
        y = y,
        colour = measure) +
    geom_line() +
    geom_vline(xintercept = c(x_max_measure1, x_max_measure2), linetype = "dashed", color = "gray") +
    geom_hline(yintercept = c(y_max_measure1, y_max_measure2), linetype = "dashed", color = "gray") +
    labs(caption = txt_tot) +
    theme(plot.caption = element_text(hjust = 0)) +
    xlim(c(0, 1)) +
    ylab("") +
    xlab("Threshold")
  
  return(gg)
}

# creating classes according to score and cut
fun_cut_predict = function(score, cut) {
  # score: predicted scores
  # cut: threshold for classification
  
  classes = score
  classes[classes > cut] = 1
  classes[classes <= cut] = 0
  classes = as.factor(classes)
  
  return(classes)  
}

# computing AUPR
aucpr = function(obs, score){
  # obs: real classes
  # score: predicted scores
  
  df = data.frame("pred" = score,
                  "obs" = obs)
  
  prc = pr.curve(df[df$obs == 1, ]$pred,
                 df[df$obs == 0, ]$pred)
  
  return(prc$auc.davis.goadrich)
}

# plotting PR curve
gg_prcurve = function(df) {
  # df: df containing models scores by columns and the last column must be
  #     nammed "obs" and must contain real classes.
  
  # init
  df_gg = data.frame("v1" = numeric(), 
                     "v2" = numeric(), 
                     "v3" = numeric(), 
                     "model" = character(),
                     stringsAsFactors = F)
  
  # individual pr curves
  for (i in c(1:(ncol(df)-1))) {
    x1 = df[df$obs == 1, i]
    x2 = df[df$obs == 0, i]
    prc = pr.curve(x1, x2, curve = T)
    
    df_prc = as.data.frame(prc$curve, stringsAsFactors = F) %>% 
      mutate(model = colnames(df)[i])
    
    # combining pr curves
    df_gg = bind_rows(df_gg,
                      df_prc)
    
  }
  
  gg = df_gg %>% 
    ggplot() +
    aes(x = V1, y = V2, colour = model) +
    geom_line() +
    xlab("Recall") +
    ylab("Precision")
  
  return(gg)
}
```
```{r}
bank_data = read.table("bank-additional-full.csv",header=TRUE,sep=";")
```
#eda
* basic information
The dataset has 41,188 rows and 21 columns. The first 20 variables are our potential explanatory variables, and the last one "y" is the dependent variable.
Among those potential explanatory variables, there are seven variables related to the bank client data: Age, job, marital, education, default, housing, loan. Four variables are related to the last contact of the current campaign: contact, month, day of week, and duration. 4 variables are related to other attributes:campaign, pdays, previous, and poutcome. And five variables are related to social and economic context attributes: Emp.var.rate, Cons.price.idx, Cons.conf.idx, Euribor3m, and Nr.employed.

* missing patten
There are 12,718 unknown values in the dataset, six features (default, education, housing, loan, job, marital) have at least one unknown value. Four features (default, education, housing, loan) with more than 330 missing values, which we cannot afford. Thus, we eliminate these four variables. Moreover, for the rest of the two (job and marital), we delete the observation with missing values in them. Moreover, Since the goal is to seek the best candidates who will have the best odds to subscribe to a term deposit, the call duration cannot be known before. So this feature is removed from data.

```{r, warning=F,fig.height=5 , fig.width=10}
bank_data %>% 
  summarise_all(list(~sum(. == "unknown"))) %>% 
  gather(key = "variable", value = "nr_unknown") %>% 
  arrange(-nr_unknown)
```

* Multicollinearity
Considering that there are five continuous variables of social and economic indicators, the figures show three pairs show a high correlation coefficient, which means they share redundant information. 

```{r, warning=F,fig.height=5 , fig.width=10}
bank_data %>% 
  select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed) %>% 
  cor() %>% 
  corrplot(method = "number",
           type = "upper",
           tl.cex = 0.8,
           tl.srt = 45,
           tl.col = "black")
```



The following chart shows the evolution of "y = 1" frequencies according to each social and economic variable.The figure x implies that the emp.var.rate is not meaningful. We are removing it to soften correlations between those five variables

```{r, warning=F,fig.height=5 , fig.width=10}
#gg_emp.var.rate = fun_gg_freq("emp.var.rate")
#gg_cons.price.idx = fun_gg_freq("cons.price.idx")
#gg_cons.conf.idx = fun_gg_freq("cons.conf.idx")
#gg_euribor3m = fun_gg_freq("euribor3m")
#gg_nr.employed = fun_gg_freq("nr.employed")

#plot_grid(gg_emp.var.rate + theme(legend.position = "none") + ylab("Frequency"), 
       #   gg_cons.price.idx + theme(legend.position = "none"),
      #   gg_cons.conf.idx + theme(legend.position = "none"),
      #    gg_euribor3m + theme(legend.position = "none"),
      #    gg_nr.employed + theme(legend.position = "none"),
      #    get_legend(gg_cons.conf.idx),
      #    align = "vh")

```
This is an unbalanced two-levels categorical variable, 88.7% of values taken are "no" (or "0") and only 11.3% of the values are "yes" (or "1"). It is more natural to work with a 0/1 dependent variable:
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  mutate(y = factor(if_else(y == "yes", "1", "0"), 
                    levels = c("0", "1")))
```
*
Considering other potential variables, we notice that the distribution of age has a special pattern; 2.21% of the total people are the elder(older than 60 years). However,elderly persons represent 8.92% of observations, which agreed to subscribe to a term deposit, and this proportion decreases to 1.36% for non-subscribers.It reflects that although the 60-years has a higher probability of subscribing to a term deposit, the banks are not very much interested in contacting them. Since the pattern among the elder and no clear pattern among others, we slice the age feature at 15 years in the young, and sum the population over 60 years old into one group to make four easily interpretable classes : [17, 30], [30, 45],[45,60] and [60, 98].The minimum and maximum values are 17 and 98. We are replacing the continuous variable "age" by this categorical variable.
We might lose some information from this continuous-to-discrete transformation, but there was not any clear pattern between years. Cutting into classes make the algorithms easier to interpret later.
```{r, warning=F,fig.height=5 , fig.width=10}
#Age divided into groups
#First of all, the banks are not very much interested by contacting the older populatio
bank_data %>% 
  ggplot() +
  aes(x = age) +
  geom_bar() +
  geom_vline(xintercept = c(15,30,45,60), 
             col = "red",
             linetype = "dashed") +
  facet_grid(y ~ .,
             scales = "free_y") +
  scale_x_continuous(breaks = seq(0, 100, 5))
```

Calling more than ten times the same person during a single marketing campaign seems excessive. We will consider those as outliers, even if marketing harassment a real thing. However, we can see that on the chart that harassment is not working at all.
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data %>% 
  ggplot() +
  aes(x = campaign) +
  geom_bar() +
  facet_grid(y ~ .,
             scales = "free_y") +
  scale_x_continuous(breaks = seq(0, 50, 5))

```
In brief, after the data exploration analysis, we keep 14 potential variables to make subsequent model selection. They are age, job, marital, contact, month, day_of_week, campaign, previous, outcome, cons.price.idx, cons.conf.idx, euribor3m, nr.employed,and pdays_dummy.


```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  mutate(age = if_else(age > 60, "above60", 
                       if_else(age > 45, "froms45to60",
                               if_else(age > 30, "froms30to45",
                                       if_else(age > 15, "below30","null")))))
bank_data$age<-as.factor(bank_data$age)

```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  filter(job != "unknown")
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  filter(marital != "unknown")
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  select(-education)
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  select(-default)
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  select(-housing)
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  select(-loan)
```
```{r, warning=F,fig.height=5 , fig.width=10}
month_recode = c("jan" = "(01)jan",
                 "feb" = "(02)feb",
                 "mar" = "(03)mar",
                 "apr" = "(04)apr",
                 "may" = "(05)may",
                 "jun" = "(06)jun",
                 "jul" = "(07)jul",
                 "aug" = "(08)aug",
                 "sep" = "(09)sep",
                 "oct" = "(10)oct",
                 "nov" = "(11)nov",
                 "dec" = "(12)dec")

bank_data = bank_data %>% 
  mutate(month = recode(month, !!!month_recode))
```
```{r, warning=F,fig.height=5 , fig.width=10}
day_recode = c("mon" = "(01)mon",
               "tue" = "(02)tue",
               "wed" = "(03)wed",
               "thu" = "(04)thu",
               "fri" = "(05)fri")

bank_data = bank_data %>% 
  mutate(day_of_week = recode(day_of_week, !!!day_recode))
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  select(-duration)
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  filter(campaign <= 10)
bank_data$campaign<-as.factor(bank_data$campaign)
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  mutate(campaign = as.character(campaign))
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  mutate(pdays_dummy = if_else(pdays == 999, "0", "1")) %>% 
  select(-pdays)
bank_data$pdays_dummy<-as.factor(bank_data$pdays_dummy)
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  mutate(previous = if_else(previous >=  2, "2+", if_else(previous == 1, "1", "0")))
bank_data$previous<-as.factor(bank_data$previous)
```
```{r, warning=F,fig.height=5 , fig.width=10}
bank_data = bank_data %>% 
  select(-emp.var.rate)


```
```{r, warning=F,fig.height=5 , fig.width=10}
set.seed(1)

ind = createDataPartition(bank_data$y,
                          times = 1,
                          p = 0.8,
                          list = F)
bank_train = bank_data[ind, ]

bank_test = bank_data[-ind, ]
summary(bank_data)

```

# 3. Predictive Models
## 3.1 Logistic regression

### Model
In this study, we want to build an appropriate prediction model for potential new users to a long-term deposit. Since logical regression is used in marketing applications, such as the prediction of a customer's propensity to purchase a product or halt a subscription[citation: Berry, Michael J.A (1997). Data Mining Techniques For Marketing, Sales, and Customer Support. Wiley. p. 10.], we first consider using the Logistical regression model for prediction.

$\ell=log\frac{p}{1-p}=\beta_0+\beta_ix_i,i=1,2,3...$

In this study, we have one binary response variable Y (whether a user to the long-term deposit).

p: P(Y=1)

$\beta_0$: y-intercept 

$\beta_i$: regression coefficient of ith variable

$x_i$: the ith potential predictor variable.

### Model Fitting

* Initial model

According to the result of exploratory data analysis, we build the initial model, with 14 predictor variables.
```{r,include=F}
logistic = glm(y ~ .,
               data = bank_train,
               family = "binomial")
summary(logistic) #table x
```

From the summary information of the initial model, we find that many features are non-significant in this model. Thus, We use step function to choose significant variables for the final model. After that, we drop the variables which are not relevant to our model and obtain the new model, which contains variable age, job, contact, month, day_of_week, campaign, outcome, cons.conf.idx, nr.employed, and pdays.

```{r,include=F}
logistic_2 = step(object=logistic,trace=0)
summary(logistic_2)
```

* Final model

We also rank the importance of these variables/features (Figure x). From this figure, we can see that although from the result of step function, job and campaign are significant variables, many of their level don not show too much importance while other variables are relevant to our models. As a consequence, we drop these two variables to get the final model.

Figure X
```{r,echo=F}
fun_imp_ggplot_split(logistic_2) #Figure x
```


```{r}
#for HL-test
logistic_3 = glm(y ~ age + contact + month + day_of_week + poutcome + cons.conf.idx + nr.employed + pdays_dummy,  family = "binomial", data = bank_train)
# summary(logistic_2)
```

After that, predict scores are computed for better evaluating and validating the final model. Since the prediction of a logistic regression model is a probability, we have to choose a cutoff value (threshold value) to use it as a classifier. The default threshold is 0.5. Nevertheless, in our study, the 0.2 cut seems a better settlement (figure X).

```{r,include=F}
logistic_train_score_3 = predict(logistic_3,
                                 newdata = bank_train,
                                 type = "response")
logistic_test_score_3 = predict(logistic_3,
                                newdata = bank_test,
                                type = "response")
```
Figure X
```{r,echo=F}
### Cut identification  #figure X
measure_train_3 = fun_gg_cutoff(logistic_train_score_3, bank_train$y, 
                                "acc", "f")
measure_train_3 +
  geom_vline(xintercept = c(0.2, 0.5), 
             linetype = "dashed")
```
#acc: accuracy rates (for accuracy); f: F1 rates (for precision). We want to have a good F1 score without dropping too much on accuracy (trade-off), and the 0.2 cut seems a good settlement.

To summarize the prediction results on a classification problem, a confusion matrix is needed. It gives us insight not only into the errors being made by a classifier but, more importantly, the types of errors that are being made. 

From Table X, we know that on the training set, the accuracy of the final logistic regression model reaches 86.76% and the Sensitivity rate is close to 56.72%, which means that model manages to correctly label 86.76% of the times and 56.72% of the willing customers are correctly detected. 

Table X

| Accuracy | Sensitivity |
|----------|-------------|
| 0.8676 | 0.56717 |
```{r,include=F}
logistic_train_cut_3 = 0.2 #前面选择的cutoff数值
logistic_train_class_3 = fun_cut_predict(logistic_train_score_3, logistic_train_cut_3)
# matrix
logistic_train_confm_3 = confusionMatrix(logistic_train_class_3, bank_train$y, 
                                         positive = "1",
                                         mode = "everything")
logistic_train_confm_3
```
To evaluate this model, we analyze the hold out(validation/test) data. The same procedures are executed, according to figure X, 0.2 is also chosen as the cutoff value. The performance values are close to the training ones; our model does not suffer from over-fitting (Table X).

Figure X
```{r,echo=F}
measure_test_3 = fun_gg_cutoff(logistic_test_score_3, bank_test$y, 
                               "acc", "f")
measure_test_3 +
  geom_vline(xintercept = c(logistic_train_cut_3, 0.5), 
             linetype = "dashed")
```

Table X

| Accuracy | Sensitivity |
|----------|-------------|
| 0.8639 | 0.53231 |

```{r,include=F}
logistic_test_class_3 = fun_cut_predict(logistic_test_score_3, logistic_train_cut_3)
# matrix
logistic_test_confm_3 = confusionMatrix(logistic_test_class_3, bank_test$y, 
                                       positive = "1",
                                        mode = "everything")
logistic_test_confm_3
```

#Model diagnostic

For the logistic regression model, the model diagnostic should cover the check of the goodness of fit, influential outliers, multicollinearity. To check the goodness of fit, we use the Hosmer-Lemeshow test (the HL-test), which could be used for the model, which contains continuous variables. The HL-test result shows the p-value is 0.001203, slightly more significant than the significance level $\alpha$=0.001. Thus, the null hypothesis that the observed and expected proportions are the same across all doses cannot be directly rejected (However, it is also hard to say accept the null hypothesis).
```{r, include=F}
#Goodness of fit
#anova chi-square test
anova(object= logistic_3,test='Chisq')
#HL-test
library(ResourceSelection)
HL_test <- hoslem.test(logistic_3$y,fitted(logistic_3),g=10)
HL_test
```

To check whether there is an influential outlier, we first draw the Cook's distance plot (Figure X). Since not all outliers are influential, we need to inspect the standardized residual error. Data points with an absolute standardized residuals above 3 represent possible influential outliers. From the calculation result, we know that there is no influential outlier.

Figure x
```{r, echo=F}
library(broom)
#influential values
plot(logistic_3, which = 4, id.n = 3)
```

```{r,include=F}
# Extract model results
model.data <- augment(logistic_3) %>% 
  mutate(index = 1:n())

model.data %>% top_n(3, .cooksd)

model.data %>% 
  filter(abs(.std.resid) > 3)
```

To check multicollinearity, we calculate the GVIF of these variables (Table X). Set GVIF>=10 as the threshold of strong multicollinearity. From Table X, we figure out that the GVIF of the outcome is greater than 10. However, consider that the outcome is a dummy variable; the interpretation of GVIF is different from VIF. In this case, we refer to GVIF^(1/(2*Df)).   
Since GVIF^(1/(2*Df))=2 is usually considered to be equivalent to VIF=4, we can say that these variables do not have any strong multicollinearity.

#Table X


|  | GVIF | Df | GVIF^(1/(2*Df)) |
|---------------|-----------|----|-----------------|
| age | 1.156307 | 3 | 1.024500 |
| contact | 1.436606 | 1 | 1.198585 |
| month | 3.716152 | 9 | 1.075652 |
| day_of_week | 1.037345 | 4 | 1.004594 |
| poutcome | 10.977510 | 2 | 1.820229 |
| cons.conf.idx | 1.975128 | 1 | 1.405393 |
| nr.employed | 2.084442 | 1 | 1.443760 |
| pdays_dummy | 9.775361 | 1 | 3.126557 |

```{r,include=F}
#multicollinearity
# we suggested using GVIF^(1/(2*Df)), where Df is the number of coefficients in the subset. In effect, this reduces the GVIF to a linear measure, and for the VIF, where Df = 1, is proportional to the inflation due to collinearity in the confidence interval for the coefficient.
car::vif(logistic_3) #Table X
```



3.2  Random Forest

Decision tree is one of the common predictive models. In this paper, to compare the prediction ability of logistic regression model and decision tree, data is set with discrete-value target variables. Thus, a classification tree is proposed. However, when the classification tree depends on many of the irrelevant features of the training data, its predictive power will reduce due to overfitting. (Bramer, 2013) To solve the overfitting problem, random forests are build instead of a single classification tree. Because of the Law of Large Numbers, random forests do not overfit, and the proof is shown by Breiman in 2001.

This paper uses ranger random forest algorithm, which utilizes the memory in an efficient manner with less runtime comparing to other random forest algorithms. (Rao, et al., 2020) In ranger model, the tuning parameters are number of variables randomly sampled as candidates at each split (mtry), splitting rule, and minimal node size. As mtry value is usually set as a third of the number of variables by default, the maximum value of mtry is set to 70% of variables in this paper, and all potential mtry values are saved in a sequence to be compared. When choosing the splitting rules, considering random forests are built based on classification trees, gini index, extra trees, and hellinger distance are considered. In addition, hellinger distance can be a better measurement among these three rules due to the imbalanced data set. (Chaabane, et al., 2019) In order to reduce the runtime, the minimal node size is set to constant one. Figure *** shows the cross validation scores comparing the different mtry values and splitting rules. Table *** also indicates that to attain the best prediction result, mtry value is set to be 3 with gini index as the splitting rule.

Figure *** provides the visualization of importance score among all variables, and the result shows a slight difference comparing to the significant variables selected by logistic regression.
Similar as testing the sensitivity and specificity of logistic regression model, the cut off point is chosen as 0.3. Comparing to logistic regression model, the accuracy, sensitivity, and specificity are all improved by random forests. (Table ***) In next section, the prediction ability of logistic regression and random forests will be compared in detail.
































Reference

Bramer, M. (2013). Avoiding overfitting of decision trees, Principles of Data Mining, Springer, London, 121-136.
Breiman, L. (2001). Random forest, Machine Learning (45), 5-32.
Rao G.M., Ramesh D., Kumar A. (2020) RRF-BD: Ranger random forest algorithm for big data classification. Computational Intelligence in Data Mining. Advances in Intelligent Systems and Computing, vol 990. Springer, Singapore, 15-25.
Chaabane, I, Guermazi, R., Hammami, M. (2019) Enhancing techniques for learning decision trees from imbalanced data, Springer.
